{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1gursJKo8hX0JYtWY/IZH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DenuraTHEbest/InvestHere/blob/Amna_S/aspiFinal2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "svXXtXkBzo40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 922
        },
        "outputId": "e6c895c2-6f76-4beb-f363-1b4ae1b75a5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Input feature columns: ['Open', 'High', 'Low', 'Vol.', 'Change %', 'Price_Lag1', 'Vol_Lag1', 'Price_MA_5', 'Price_MA_10', 'Price_MA_20', 'Price_Volatility_5', 'Price_Volatility_20', 'Lagged_Sentiment_1', 'Lagged_Sentiment_2', 'Lagged_Sentiment_3', 'Lagged_Sentiment_4', 'Lagged_Sentiment_5', 'Price_Lag10', 'Vol_Lag10', 'Price_Lag20', 'Vol_Lag20', 'Price_Lag30', 'Vol_Lag30']\n",
            "Training Data: (601, 23), Testing Data: (151, 23)\n",
            "Model training completed.\n",
            "MAE for day 1: 15.147444370860917\n",
            "MAE for day 2: 92.14205960264667\n",
            "MAE for day 3: 90.28729801324303\n",
            "MAE for day 4: 94.11947218543091\n",
            "MAE for day 5: 111.4937662251671\n",
            "MAE for day 6: 145.63355231788046\n",
            "MAE for day 7: 122.31557880794668\n",
            "MAE for day 8: 119.09969735099291\n",
            "MAE for day 9: 130.63780993377426\n",
            "MAE for day 10: 121.62661721854315\n",
            "MAE for day 11: 111.0879503311265\n",
            "MAE for day 12: 124.74777350993394\n",
            "MAE for day 13: 191.29643046357563\n",
            "MAE for day 14: 261.17315894039615\n",
            "MAE for day 15: 239.61621125827887\n",
            "MAE for day 16: 263.5401066225141\n",
            "MAE for day 17: 325.25633377483626\n",
            "MAE for day 18: 309.3163059602662\n",
            "MAE for day 19: 300.4359000000008\n",
            "MAE for day 20: 334.96394635761504\n",
            "MAE for day 21: 320.5359456953644\n",
            "MAE for day 22: 321.4473556291387\n",
            "MAE for day 23: 348.29636158940394\n",
            "R2 for day 1: 0.9987230812458134\n",
            "R2 for day 2: 0.9598247859098022\n",
            "R2 for day 3: 0.9630011406054837\n",
            "R2 for day 4: 0.9583440678168542\n",
            "R2 for day 5: 0.9258300602897663\n",
            "R2 for day 6: 0.8926005065842414\n",
            "R2 for day 7: 0.9421197756739261\n",
            "R2 for day 8: 0.9447773665875366\n",
            "R2 for day 9: 0.933990060304022\n",
            "R2 for day 10: 0.9420771654590809\n",
            "R2 for day 11: 0.9490959792962596\n",
            "R2 for day 12: 0.9349555737368858\n",
            "R2 for day 13: 0.8590716559852745\n",
            "R2 for day 14: 0.7488412859576128\n",
            "R2 for day 15: 0.7626492344104522\n",
            "R2 for day 16: 0.7656680230310793\n",
            "R2 for day 17: 0.7096286127946151\n",
            "R2 for day 18: 0.7101795323823228\n",
            "R2 for day 19: 0.6943978522911027\n",
            "R2 for day 20: 0.6328534339814897\n",
            "R2 for day 21: 0.6351200311994994\n",
            "R2 for day 22: 0.6388958987236923\n",
            "R2 for day 23: 0.608059865600935\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_066f4a62-ce6c-4334-bc0e-6d0a3dd10e35\", \"aspi_forecast_model.pkl\", 126257969)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved and downloaded successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import numpy as np\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load datasets\n",
        "aspi_data = pd.read_csv('/content/drive/My Drive/CSE_All_Share_Historical_Data_Processed.csv')\n",
        "sentiment_data = pd.read_csv('/content/drive/My Drive/daily_weighted_scores_9-12.csv')\n",
        "\n",
        "# Convert the Date columns to pandas datetime for both datasets\n",
        "aspi_data['Date'] = pd.to_datetime(aspi_data['Date'])  # Already in YYYY-MM-DD format\n",
        "sentiment_data['Date'] = pd.to_datetime(sentiment_data['Date'])  # Already in YYYY-MM-DD format\n",
        "\n",
        "# Filter for the common date range\n",
        "common_start_date = max(aspi_data['Date'].min(), sentiment_data['Date'].min())\n",
        "common_end_date = min(aspi_data['Date'].max(), sentiment_data['Date'].max())\n",
        "\n",
        "aspi_data = aspi_data[(aspi_data['Date'] >= common_start_date) & (aspi_data['Date'] <= common_end_date)]\n",
        "sentiment_data = sentiment_data[(sentiment_data['Date'] >= common_start_date) & (sentiment_data['Date'] <= common_end_date)]\n",
        "\n",
        "# Sort both datasets by date in ascending order\n",
        "aspi_data = aspi_data.sort_values(by='Date')\n",
        "sentiment_data = sentiment_data.sort_values(by='Date')\n",
        "\n",
        "# Merge datasets with lagged sentiment features\n",
        "for lag in range(1, 6):  # 5 days lag\n",
        "    sentiment_data[f'Lagged_Sentiment_{lag}'] = sentiment_data['Weighted_Sentiment'].shift(lag)\n",
        "\n",
        "# Drop rows with NaN values introduced by shifting\n",
        "sentiment_data = sentiment_data.dropna(subset=[f'Lagged_Sentiment_{lag}' for lag in range(1, 6)])\n",
        "\n",
        "# Merge the sentiment data with the ASPI data\n",
        "merged_data = pd.merge(aspi_data, sentiment_data[['Date'] + [f'Lagged_Sentiment_{lag}' for lag in range(1, 6)]],\n",
        "                       on='Date', how='left')\n",
        "\n",
        "# Create lagged price and volume features\n",
        "merged_data['Price_Lag10'] = merged_data['Price'].shift(10)\n",
        "merged_data['Vol_Lag10'] = merged_data['Vol.'].shift(10)\n",
        "merged_data['Price_Lag20'] = merged_data['Price'].shift(20)\n",
        "merged_data['Vol_Lag20'] = merged_data['Vol.'].shift(20)\n",
        "merged_data['Price_Lag30'] = merged_data['Price'].shift(30)\n",
        "merged_data['Vol_Lag30'] = merged_data['Vol.'].shift(30)\n",
        "\n",
        "# Create target variables for the next 23 days\n",
        "for i in range(1, 24):\n",
        "    merged_data[f'Target_{i}'] = merged_data['Price'].shift(-i)\n",
        "\n",
        "# Remove rows with NaN values (from shifting)\n",
        "merged_data = merged_data.dropna()\n",
        "\n",
        "# Features (X) and Target (y)\n",
        "X = merged_data.drop(columns=['Date', 'Price'] + [f'Target_{i}' for i in range(1, 24)])\n",
        "y = merged_data[[f'Target_{i}' for i in range(1, 24)]]\n",
        "\n",
        "print(\"Input feature columns:\", X.columns.tolist())\n",
        "\n",
        "# Split the data into train (80%) and test (20%) while preserving time order\n",
        "split_ratio = 0.8\n",
        "split_index = int(len(X) * split_ratio)\n",
        "\n",
        "X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]\n",
        "y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]\n",
        "\n",
        "print(f\"Training Data: {X_train.shape}, Testing Data: {X_test.shape}\")\n",
        "\n",
        "# Initialize and train Multi-Output RandomForestRegressor\n",
        "base_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model = MultiOutputRegressor(base_model)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Model training completed.\")\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate model performance using Mean Absolute Error (MAE) for each day ahead\n",
        "mae_scores = [mean_absolute_error(y_test.iloc[:, i], y_pred[:, i]) for i in range(y_test.shape[1])]\n",
        "\n",
        "# Evaluate model performance using r2\n",
        "from sklearn.metrics import r2_score\n",
        "r2_scores = [r2_score(y_test.iloc[:, i], y_pred[:, i]) for i in range(y_test.shape[1])]\n",
        "\n",
        "# Print MAE results\n",
        "for i, mae in enumerate(mae_scores, 1):\n",
        "    print(f\"MAE for day {i}: {mae}\")\n",
        "\n",
        "for i, r2 in enumerate(r2_scores, 1):\n",
        "    print(f\"R2 for day {i}: {r2}\")\n",
        "\n",
        "# Save the trained model\n",
        "joblib.dump(model, \"aspi_forecast_model.pkl\")\n",
        "\n",
        "# Download the model to local machine\n",
        "from google.colab import files\n",
        "files.download(\"aspi_forecast_model.pkl\")\n",
        "\n",
        "print(\"Model saved and downloaded successfully!\")\n"
      ]
    }
  ]
}